{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is largely copied\n",
    "see: https://www.kaggle.com/fabiendaniel/hyperparameter-tuning/notebook\n",
    "\n",
    "We do need to change to make it our own.\n",
    "\n",
    "# Team 3 - Merchant Category Recommendation\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "### Team 3\n",
    "- Vinicio De Sola\n",
    "- Kevin Hanna\n",
    "- Pri Nonis\n",
    "- Bradley Nott\n",
    "\n",
    "\n",
    "Here we add engineer new features and write the files out to:\n",
    "    - input/engineered_train.pkl\n",
    "    - input/engineered_test.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', 'Data_Dictionary.xlsx', 'engineered_test.pkl', 'engineered_train.pkl', 'historical_transactions.csv', 'merchants.csv', 'new_merchant_transactions.csv', 'sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import pandas              as pd\n",
    "import seaborn             as sb\n",
    "\n",
    "from datetime              import timedelta, datetime\n",
    "\n",
    "from sklearn               import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.mixture       import GaussianMixture\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "\n",
    "from matplotlib.colors     import LogNorm\n",
    "\n",
    "from IPython.display       import HTML, Markdown\n",
    "\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_pickle('input/engineered_train.pkl')\n",
    "test   = pd.read_pickle('input/engineered_test.pkl' )\n",
    "\n",
    "target = train['target']\n",
    "del      train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A view of our training data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())\n",
    "display(test.head())\n",
    "display(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df) :\n",
    "    pre = df.memory_usage().sum() / 1024**2 / 8\n",
    "    cmp = {'f' : {np.finfo(np.float16).max : np.float16,\n",
    "                  np.finfo(np.float32).max : np.float32,\n",
    "                  np.finfo(np.float64).max : np.float64},\n",
    "           'i' : {np.iinfo(np.int8   ).max : np.int8,\n",
    "                  np.iinfo(np.int16  ).max : np.int16,\n",
    "                  np.iinfo(np.int32  ).max : np.int32,\n",
    "                  np.iinfo(np.int64  ).max : np.int64}}\n",
    "\n",
    "    for c in df.columns :\n",
    "        if  cmp.get(df[c].dtype.kind) :\n",
    "            df[c] = df[c].astype(cmp[df[c].dtype.kind].get(min((n for n in cmp[df[c].dtype.kind].keys() if n > max(df[c].max(), abs(df[c].min()))))))\n",
    "\n",
    "    end = df.memory_usage().sum() / 1024**2 / 8\n",
    "\n",
    "    print(f'Memory Use Decreased to {end:5.2f} MB [{100 * (pre - end) / pre:.1f}% Reduction]')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  2.82 Mb (-7.3% reduction)\n",
      "                        feature_1_1 : uint8 to float16 != uint8\n",
      "                        feature_1_2 : uint8 to float16 != uint8\n",
      "                        feature_1_3 : uint8 to float16 != uint8\n",
      "                        feature_1_4 : uint8 to float16 != uint8\n",
      "                        feature_1_5 : uint8 to float16 != uint8\n",
      "                        feature_2_1 : uint8 to float16 != uint8\n",
      "                        feature_2_2 : uint8 to float16 != uint8\n",
      "                        feature_2_3 : uint8 to float16 != uint8\n"
     ]
    }
   ],
   "source": [
    "train = compress(train)\n",
    "\n",
    "for x in train.columns :\n",
    "    if  mine[x] != train[x].dtype :\n",
    "        print(f'{x:>35} : {orig[x]} to {train[x].dtype} != {mine[x]}')\n",
    "\n",
    "features = train.columns\n",
    "categorical_feats = [c for c in features if 'feature_' in c]\n",
    "\n",
    "\n",
    "# target = reduce_mem_usage(target)\n",
    "# features = reduce_mem_usage(features)\n",
    "# categorical_feats = reduce_mem_usage(categorical_feats)\n",
    "# categorical_feats = categorical_feats.astype('bool')\n",
    "\n",
    "# display(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_CV(\n",
    "          max_depth,\n",
    "          num_leaves,\n",
    "          min_data_in_leaf,\n",
    "          feature_fraction,\n",
    "          bagging_fraction,\n",
    "          lambda_l1\n",
    "         ):\n",
    "    \n",
    "    ### Delete this\n",
    "   \n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(train.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold nÂ°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n",
    "                               label=target.iloc[trn_idx],\n",
    "                               categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features],\n",
    "                               label=target.iloc[val_idx],\n",
    "                               categorical_feature=categorical_feats)\n",
    "    \n",
    "        param = {\n",
    "            'num_leaves': int(num_leaves),\n",
    "            'min_data_in_leaf': int(min_data_in_leaf), \n",
    "            'objective':'regression',\n",
    "            'max_depth': int(max_depth),\n",
    "            'learning_rate': 0.01,\n",
    "            \"boosting\": \"gbdt\",\n",
    "            \"feature_fraction\": feature_fraction,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"bagging_fraction\": bagging_fraction ,\n",
    "            \"bagging_seed\": 11,\n",
    "            \"metric\": 'rmse',\n",
    "            \"lambda_l1\": lambda_l1,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "    \n",
    "        clf = lgb.train(param,\n",
    "                        trn_data,\n",
    "                        10000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        verbose_eval=500,\n",
    "                        early_stopping_rounds = 200)\n",
    "        \n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features],\n",
    "                                   num_iteration=clf.best_iteration)\n",
    "        \n",
    "        del clf, trn_idx, val_idx\n",
    "        gc.collect()\n",
    "        \n",
    "    return -mean_squared_error(oof, target)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "LGB_BO = BayesianOptimization(LGB_CV, {\n",
    "    'max_depth': (4, 10),\n",
    "    'num_leaves': (5, 130),\n",
    "    'min_data_in_leaf': (10, 150),\n",
    "    'feature_fraction': (0.7, 1.0),\n",
    "    'bagging_fraction': (0.7, 1.0),\n",
    "    'lambda_l1': (0, 6)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod(\n",
    "            (datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('-'*126)\n",
    "\n",
    "start_time = timer(None)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=2, n_iter=20, acq='ei', xi=0.0)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
