{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy   as np # linear algebra\n",
    "import pandas  as pd # data processing\n",
    "import os.path as op # file system access\n",
    "import os      as os\n",
    "import gc      as gc\n",
    "import time    as ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, verbose = True) :\n",
    "\n",
    "    smu = df.memory_usage().sum() / 1024**2 / 8\n",
    "    con = {'f' : {                                   np.finfo(np.float16).max : np.float16, np.finfo(np.float32).max : np.float32, np.finfo(np.float64).max : np.float64},\n",
    "           'u' : {np.iinfo(np.uint8).max : np.uint8, np.iinfo(np.uint16).max  : np.uint16,  np.iinfo(np.uint32).max  : np.uint32,  np.iinfo(np.uint64).max  : np.uint64},\n",
    "           'i' : {np.iinfo(np.int8).max  : np.int8,  np.iinfo(np.int16).max   : np.int16,   np.iinfo(np.int32).max   : np.int32,   np.iinfo(np.int64).max   : np.int64}}\n",
    "\n",
    "    for c in df.columns :\n",
    "        if  con.get(df[c].dtype.kind) :\n",
    "            df[c] = df[c].astype(con[df[c].dtype.kind].get(min((n for n in con[df[c].dtype.kind].keys() if n > max(df[c].max(), abs(df[c].min()))))))\n",
    "\n",
    "    emu = df.memory_usage().sum() / 1024**2 / 8\n",
    "\n",
    "    if  verbose :\n",
    "        print(f'Memory Use Decreased to {emu:5.2f} MB [{100 * (smu - emu) / emu:5.1f}% Reduction]')\n",
    "\n",
    "    return df, 100 * (smu - emu) / emu\n",
    "\n",
    "def read(csv_path, dates = [], brize = [], dummy = [], delna = False, index = None, repkl = False) :\n",
    "    \n",
    "    pkl_path = csv_path.replace('.csv', '.pkl').replace('../input', '../pickle')\n",
    "    srt_time = ti.time()\n",
    "    \n",
    "    if  op.exists(pkl_path) and not repkl :\n",
    "\n",
    "        df       = pd.read_pickle(pkl_path)\n",
    "        csv_path = pkl_path\n",
    "        rp       = 0.0\n",
    "\n",
    "    else                    :\n",
    "\n",
    "        df = pd.read_csv(csv_path, parse_dates = dates, memory_map = True)\n",
    "        \n",
    "        if  index :\n",
    "            df = df.set_index(index)\n",
    "            \n",
    "        if  delna :\n",
    "            df = df.dropna()\n",
    "\n",
    "        for c in brize :\n",
    "            df[c] = df[c].eq('Y').mul(1)\n",
    "    \n",
    "        if  dummy :\n",
    "            df = pd.get_dummies(df, columns = dummy)\n",
    "        \n",
    "        try    : df, rp = compress(df, verbose = False)\n",
    "        except :     rp = -1\n",
    "\n",
    "        os.makedirs('../pickle', exist_ok = True)\n",
    "        df.to_pickle(pkl_path)\n",
    "\n",
    "    print(f'Loaded : {op.basename(csv_path):>29} in {ti.time()-srt_time:5.1f} Seconds, Shape is {str(df.shape):>14}, Memory Usage is {df.memory_usage().sum() / 1024**2 / 8:6.2f} MB [Reduction of {rp:5.1f} %].')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load() :\n",
    "\n",
    "    data           = {}\n",
    "\n",
    "    data['train' ] = read('../input/train.csv', dates = ['first_active_month'],\n",
    "                                                delna = True,\n",
    "                                                index = 'card_id',\n",
    "                                                repkl = True)\n",
    "    data['test'  ] = read('../input/test.csv',  dates = ['first_active_month'],\n",
    "                                                delna = True,\n",
    "                                                index = 'card_id',\n",
    "                                                repkl = True)\n",
    "\n",
    "    data['mercs' ] = read('../input/merchants.csv', index = 'merchant_id',\n",
    "                                                    repkl = True)\n",
    "\n",
    "    data['txnew' ] = read('../input/new_merchant_transactions.csv', dates = ['purchase_date'],\n",
    "                                                                    brize = ['authorized_flag', 'category_1'],\n",
    "                                                                    dummy = ['category_2', 'category_3'],\n",
    "                                                                    index = None)\n",
    "    data['txold' ] = read('../input/historical_transactions.csv',   dates = ['purchase_date'],\n",
    "                                                                    brize = ['authorized_flag', 'category_1'],\n",
    "                                                                    dummy = ['category_2', 'category_3'],\n",
    "                                                                    index = None)\n",
    "\n",
    "    data['target'] = data['train'].pop('target')\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded :                     train.csv in   0.7 Seconds, Shape is    (201917, 5), Memory Usage is   0.51 MB [Reduction of 128.6 %].\n",
      "Loaded :                      test.csv in   0.3 Seconds, Shape is    (123622, 4), Memory Usage is   0.28 MB [Reduction of 110.5 %].\n",
      "Loaded :                 merchants.csv in   4.0 Seconds, Shape is   (334696, 21), Memory Usage is   5.71 MB [Reduction of  -1.0 %].\n",
      "Loaded : new_merchant_transactions.pkl in   0.5 Seconds, Shape is  (1963031, 20), Memory Usage is  10.53 MB [Reduction of   0.0 %].\n",
      "Loaded :   historical_transactions.pkl in   5.6 Seconds, Shape is (29112361, 20), Memory Usage is 163.11 MB [Reduction of   0.0 %].\n"
     ]
    }
   ],
   "source": [
    "data = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft(data) :\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    plan                           = {}\n",
    "    plan['first_active_month_max'] = max(data['train'].first_active_month.max(),\n",
    "                                         data['test' ].first_active_month.max())\n",
    "    plan['purchase_date_max'     ] = max(data['txold'].purchase_date.max(),\n",
    "                                         data['txnew'].purchase_date.max())\n",
    "    plan['purchase_date_ref'     ] =     data['txold'].purchase_date.max()\n",
    "    plan['train_size'            ] = len(data['train'])\n",
    "    plan['train_pcnt'            ] = 0.8\n",
    "    plan['train_mask'            ] = np.random.rand(plan['train_size']) < plan['train_pcnt']\n",
    "    plan['devel_mask'            ] =               ~plan['train_mask']\n",
    "\n",
    "    data['y_train'               ] = data['target'][plan['train_mask']]\n",
    "    data['y_devel'               ] = data['target'][plan['devel_mask']]\n",
    "    \n",
    "    return data, plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,\\\n",
    "plan = draft(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_transactions(tf, prefix, repkl = False) :\n",
    "\n",
    "    pkl_path = f'../pickle/engineered_{prefix}_transactions.pkl'\n",
    "    srt_time = ti.time()\n",
    "    \n",
    "    if  op.exists(pkl_path) and not repkl :\n",
    "\n",
    "        df = pd.read_pickle(pkl_path)\n",
    "        \n",
    "    else :\n",
    "\n",
    "      # recover purchase history by denormolizing\n",
    "        tf['month_diff'    ] = (plan['purchase_date_ref'] - tf['purchase_date']) // np.timedelta64(1, 'M') \\\n",
    "                             + tf['month_lag']\n",
    "\n",
    "      # extract purchase_month from date\n",
    "        tf['purchase_month'] = tf['purchase_date'].dt.month\n",
    "\n",
    "      # convert datetime to numerical\n",
    "        tf['purchase_ndate'] = tf['purchase_date'].astype(np.int64) * 1e-9\n",
    "\n",
    "        aggregations = \\\n",
    "        {\n",
    "            'category_1'           : ['mean', 'sum'],\n",
    "\n",
    "            'category_2_1.0'       : ['mean'],\n",
    "            'category_2_2.0'       : ['mean'],\n",
    "            'category_2_3.0'       : ['mean'],\n",
    "            'category_2_4.0'       : ['mean'],\n",
    "            'category_2_5.0'       : ['mean'],\n",
    "            'category_3_A'         : ['mean'],\n",
    "            'category_3_B'         : ['mean'],\n",
    "            'category_3_C'         : ['mean'],\n",
    "\n",
    "            'merchant_id'          : ['nunique'],\n",
    "            'merchant_category_id' : ['nunique'],\n",
    "            'state_id'             : ['nunique'],\n",
    "            'city_id'              : ['nunique'],\n",
    "            'subsector_id'         : ['nunique'],\n",
    "\n",
    "            'purchase_amount'      : ['mean', 'sum', 'max', 'min', 'std'],\n",
    "            'installments'         : ['mean', 'sum', 'max', 'min', 'std'],\n",
    "            'purchase_month'       : ['mean',        'max', 'min', 'std'],\n",
    "            'purchase_ndate'       : [np.ptp,        'max', 'min'       ],\n",
    "            'month_lag'            : ['mean',        'max', 'min', 'std'],\n",
    "            'month_diff'           : ['mean'                            ]\n",
    "        }\n",
    "\n",
    "        aggregations_kev = \\\n",
    "        {\n",
    "            'purchase_amount' : ['sum', 'mean', 'median', 'min', 'max', 'std'],\n",
    "            'subsector_id'    : ['nunique']\n",
    "        }\n",
    "\n",
    "      # add aggregations\n",
    "        df = tf.groupby(['card_id']).agg(aggregations)\n",
    "        df.columns = ['_'.join((prefix,) + c) for c in df.columns.values]\n",
    "        df.reset_index(inplace = True)\n",
    "\n",
    "      # add transaction count\n",
    "        tc = tf.groupby('card_id').size().reset_index(name = f'{prefix}_transaction_count')\n",
    "        df = pd.merge(tc, df, on = 'card_id', how = 'left')\n",
    "        \n",
    "        os.makedirs('../pickle', exist_ok = True)\n",
    "        df.to_pickle(pkl_path)\n",
    "    \n",
    "    print(f'Engineered : {prefix:>5} Transactions in {ti.time()-srt_time:5.1f} Seconds.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_cards(df, aggs, plan, prefix) :\n",
    "\n",
    "    pkl_path = f'../pickle/engineered_{prefix}_cards.pkl'\n",
    "    srt_time = ti.time()\n",
    "    \n",
    "    if  op.exists(pkl_path) and not repkl :\n",
    "\n",
    "        df = pd.read_pickle(pkl_path)\n",
    "\n",
    "    else :\n",
    "    \n",
    "      # convert categorical variables to dummy/indicator, preserve original categorical variable\n",
    "        df = pd.concat([df, pd.get_dummies(df[['feature_1', 'feature_2']],\n",
    "                                    columns = ['feature_1', 'feature_2'])], axis = 1)\n",
    "\n",
    "      # normalized active days of card\n",
    "        df['active_days'] = (plan['first_active_month_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "      # add purchase amounts of card\n",
    "      # purchase_amount_old = data['txold'].groupby(['card_id'])['purchase_amount'].sum().to_frame('old_purchase_amount')\n",
    "      # purchase_amount_new = data['txold'].groupby(['card_id'])['purchase_amount'].sum().to_frame('new_purchase_amount')\n",
    "\n",
    "      # df = df.join(purchase_amount_old[purchase_amount_old.index.isin(df.index)]).fillna(0)\n",
    "      # df = df.join(purchase_amount_new[purchase_amount_new.index.isin(df.index)]).fillna(0)\n",
    "\n",
    "        if  'txnew' in aggs :\n",
    "            df = pd.merge( df, aggs['txnew'], on = 'card_id', how = 'left')\n",
    "\n",
    "        if  'txold' in aggs : \n",
    "            df = pd.merge( df, aggs['txold'], on = 'card_id', how = 'left')\n",
    "\n",
    "        os.makedirs('../pickle', exist_ok = True)\n",
    "        df.to_pickle(pkl_path)\n",
    "            \n",
    "    print(f'Engineered : {prefix:>5} Cards        in {ti.time()-srt_time:5.1f} Seconds.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer(data, plan) :\n",
    "\n",
    "    aggs          = {}\n",
    "    aggs['txnew'] = engineer_transactions(data['txnew'], prefix = 'new')\n",
    "    aggs['txold'] = engineer_transactions(data['txold'], prefix = 'old')\n",
    "\n",
    "    feat          = {}\n",
    "    feat['train'] = engineer_cards(data['train'], aggs, plan, prefix = 'train')\n",
    "    feat['test' ] = engineer_cards(data['test' ], aggs, plan, prefix = 'test' )\n",
    "  \n",
    "    del aggs\n",
    "    d = gc.collect()\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered :   new Transactions in   0.1 Seconds.\n",
      "Engineered :   old Transactions in 133.3 Seconds.\n",
      "Engineered : train Cards        in   1.8 Seconds.\n",
      "Engineered :  test Cards        in   1.2 Seconds.\n"
     ]
    }
   ],
   "source": [
    "feat = engineer(data, plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, features, plan, baseline = False) :\n",
    "    \n",
    "    from sklearn.linear_model  import LinearRegression\n",
    "    from sklearn.metrics       import mean_squared_error\n",
    "    \n",
    "    if  features == None :\n",
    "        features  = df._get_numeric_data().columns\n",
    "\n",
    "    x_train = df[features][plan['train_mask']].fillna(0)\n",
    "    x_devel = df[features][plan['devel_mask']].fillna(0)\n",
    "\n",
    "    model   = LinearRegression() \\\n",
    "                .fit(x_train, data['y_train'])\n",
    "\n",
    "    y_pred  = model.predict(x_devel)\n",
    "    \n",
    "    if  baseline :\n",
    "        plan['baseline_mse'] = mean_squared_error(data['y_devel'], y_pred)\n",
    "\n",
    "    mse     = mean_squared_error(data['y_devel'], y_pred)\n",
    "    gain    = (plan['baseline_mse'] - mse) / plan['baseline_mse'] * 100\n",
    "    \n",
    "    print(f\"Linear Regression : Mean Squared Error is {mse:6.3f} [{gain:+6.3f}]\")\n",
    "    \n",
    "    return plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression : Mean Squared Error is 14.269 [+0.000]\n",
      "Linear Regression : Mean Squared Error is 14.811 [-3.803]\n"
     ]
    }
   ],
   "source": [
    "def regression_run(feat, plan) :\n",
    "\n",
    "    regression(feat['train'], features = ['feature_1', 'feature_2', 'feature_3'], plan = plan, baseline = True ) # base features in train          - baseline\n",
    "    regression(feat['train'], features = None,                                    plan = plan, baseline = False) # all numeric engineered features - kitchen sink\n",
    "\n",
    "    return plan\n",
    "\n",
    "plan = regression_run(feat, plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(df, features, plan, alphas) :\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.linear_model    import Lasso\n",
    "    from sklearn.metrics         import mean_squared_error\n",
    "\n",
    "    if  features == None :\n",
    "        features  = df._get_numeric_data().columns\n",
    "\n",
    "    x_train = df[features][plan['train_mask']].fillna(0)\n",
    "    x_devel = df[features][plan['devel_mask']].fillna(0)\n",
    "\n",
    "    params  = [{'alpha' : alphas}]\n",
    "    folds   = 5\n",
    "\n",
    "    model   = Lasso(random_state = 0)\n",
    "    grid    = GridSearchCV(model, params, cv = folds, scoring = 'neg_mean_squared_error') \\\n",
    "                .fit(x_train, data['y_train'])\n",
    "    y_pred  = grid.best_estimator_.predict(x_devel)\n",
    "    \n",
    "    mse     = mean_squared_error(data['y_devel'], y_pred)\n",
    "    gain    = (plan['baseline_mse'] - mse) / plan['baseline_mse'] * 100\n",
    "    \n",
    "    print(f\"Lasso             : Mean Squared Error is {mse:6.3f} [{gain:+6.3f}] (Alpha = {grid.best_estimator_.alpha:8.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso             : Mean Squared Error is 14.269 [-0.000] (Alpha =  0.00100)\n",
      "Lasso             : Mean Squared Error is 14.269 [-0.001] (Alpha =  0.00281)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def lasso_run(feat, plan) :\n",
    "    \n",
    "    lasso(feat['train'], features = ['feature_1', 'feature_2', 'feature_3'], plan = plan, alphas = [0.00001, 0.001, 0.5, 10])\n",
    "    lasso(feat['train'], features = ['feature_1', 'feature_2', 'feature_3'], plan = plan, alphas = np.logspace(-4, -0.5, 30))\n",
    "    lasso(feat['train'], features = None,                                    plan = plan, alphas = [0.00001, 0.001, 0.5, 10])\n",
    "\n",
    "    return plan\n",
    "\n",
    "plan = lasso_run(feat, plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
